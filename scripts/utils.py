"""
Merge every *.json file under /data/huali_data/  (each is a list of instances)
into one big JSON list and save to /data/huali_mm/huali_corpus.json
"""
import json, os
from tqdm import tqdm
from pathlib import Path
from itertools import islice
from collections import Counter, defaultdict
from langchain.docstore.document import Document
from IPython.display import display, Markdown, Image


# CORPUS_PATH = Path("/data/huali_data")
# OUTPUT_DIR  = Path("/data/huali_mm")
# OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
# OUTPUT_FILE = OUTPUT_DIR / "huali_corpus.json"



def merge_corpus(CORPUS_PATH, OUTPUT_FILE):
    all_instances = []

    json_files = sorted(CORPUS_PATH.rglob("*.json"))  
    for book_idx, json_file in enumerate(json_files):
        with open(json_file, "r", encoding="utf-8") as f:
            try:
                items = json.load(f)          
                if isinstance(items, list):
                    for inst in items:
                        inst["book_idx"] = book_idx
                    all_instances.extend(items)
                else:
                    print(f"[Warn] {json_file} ä¸æ˜¯åˆ—è¡¨ï¼Œå·²è·³è¿‡")
            except json.JSONDecodeError as e:
                print(f"[Error] {json_file} è§£æå¤±è´¥: {e}")

    print(f"Total merged instances: {len(all_instances)}")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_instances, f, ensure_ascii=False, indent=2)

    print(f"âœ… Saved to {OUTPUT_FILE}")
    


def analyze_kb_types(kb_path):
    with open(kb_path, encoding="utf-8") as f:
        all_insts = json.load(f)

    type_counter = Counter()
    missing_img_path_idxs = []
    samples = defaultdict(list)

    for idx, inst in enumerate(tqdm(all_insts, desc="Analyzing KB instances")):
        t = inst.get("type")
        type_counter[t] += 1

        if t == "image" and not inst.get("img_path"):
            missing_img_path_idxs.append(idx)

        if len(samples[t]) < 5:
            samples[t].append(inst)

    print(f"\nä¸€å…±å‘ç° {len(type_counter)} ç§ä¸åŒçš„ typeï¼š")
    for t, cnt in type_counter.items():
        print(f"  - {t!r}: {cnt} æ¡")
    if missing_img_path_idxs:
        print(f"\næ³¨æ„ï¼šæœ‰ {len(missing_img_path_idxs)} æ¡ type='image' çš„è®°å½•ç¼ºå°‘ 'img_path'ï¼Œç¤ºä¾‹ç´¢å¼•ï¼š{missing_img_path_idxs[:10]}{'...' if len(missing_img_path_idxs)>10 else ''}")
    else:
        print("\næ‰€æœ‰ type='image' çš„è®°å½•å‡åŒ…å« 'img_path'ã€‚")

    print("\næ¯ç§ type çš„å‰ 5 ä¸ªå®ä¾‹ï¼ˆå®Œæ•´å†…å®¹ï¼‰ï¼š")
    for t, inst_list in samples.items():
        print(f"\n--- Type = {t!r} ï¼ˆå…± {type_counter[t]} æ¡ï¼‰ ---")
        for i, inst in enumerate(inst_list, start=1):
            print(f"{i}. {inst}")
 


def union_docs(docs):
    unique_docs, seen = [], set()
    for d in docs:
        key = (d.metadata["book_idx"], d.metadata["page_idx"], d.page_content.strip())
        if key not in seen:
            unique_docs.append(d)
            seen.add(key)
    docs = unique_docs
    print(f"After dedup: {len(docs)}")



def save_docs(docs, OUTPUT_DOCS= Path("/data/huali_mm/docs.json")):
    serializable = [
        {"page_content": doc.page_content, "metadata": doc.metadata}
        for doc in docs
    ]
    with open(OUTPUT_DOCS, "w", encoding="utf-8") as f:
        json.dump(serializable, f, ensure_ascii=False, indent=2)

    print(f"âœ… Saved {len(docs)} docs to {OUTPUT_DOCS}")



def load_serialized_docs(path: Path):
    with open(path, encoding="utf-8") as f:
        raw_items = json.load(f)         # list[dict]

    # é‡æ–°æ„é€ æˆ Document
    docs = [
        Document(page_content=item["page_content"],
                 metadata=item["metadata"])
        for item in raw_items
    ]
    return docs


def preview_docs_by_type(docs, n_preview=5):
    """æŒ‰ metadata['type'] åˆ†ç»„æ‰“å°å‰ n_preview ä¸ª Document"""
    buckets = defaultdict(list)
    for d in docs:
        buckets[d.metadata["type"]].append(d)
    
    for t in ["text", "image", "table", "equation"]:
        lst = buckets.get(t, [])
        print(f"\n=== {t.upper()}  (å…± {len(lst)} æ¡) ===")
        for i, d in enumerate(islice(lst, n_preview), 1):
            print(f"{i}.", d)


def block_fmt(doc: Document, idx: int) -> str:
    tp  = doc.metadata["type"]
    pg  = doc.metadata["page_idx"]
    b   = doc.metadata["book_idx"]
    head = f"[{idx:02d}] ({tp.upper()} | book={b}, page={pg})"
    body = doc.page_content.strip()
    return f"{head}\n{body}"



def render_mm_results(result, top_media_parents, response):
    # æŠŠ media tag æ˜ å°„åˆ°è·¯å¾„ & caption
    tag2info = {}
    for idx, doc in enumerate(top_media_parents, 1):
        tag = f"<MEDIA_{idx}>"
        path = doc.metadata["img_path"]
        caption = (doc.metadata.get("img_caption") or
                   doc.metadata.get("table_caption") or "")
        caption = " ".join(caption) if isinstance(caption, list) else caption
        tag2info[tag] = (path, caption)

    # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
    display(Markdown("## ğŸ¤” æ€è€ƒè¿‡ç¨‹\n\n" +
                     response.choices[0].message.reasoning_content))

    # æ˜¾ç¤ºæ¶¦è‰²åå›ç­”å¹¶æ’å…¥å›¾è¡¨
    display(Markdown("## ğŸ’¡ å›ç­”\n"))
    for para in result["enhanced_paragraphs"]:
        # é€æ®µè¾“å‡ºï¼Œæ›¿æ¢å ä½ç¬¦
        for tag, (img_path, cap) in tag2info.items():
            if tag in para:
                # æ®µè½æ–‡æœ¬å»æ‰ tag å ä½
                para_text = para.replace(tag, "").strip()
                if para_text:
                    display(Markdown(para_text))
                display(Image(img_path))
                display(Markdown(f"*{cap}*"))
                break
        else:
            # æ™®é€šæ®µè½
            display(Markdown(para))
